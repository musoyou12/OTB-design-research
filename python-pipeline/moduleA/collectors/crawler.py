"""
crawler.py - Enhanced Version
- 50+ RSS sources across domains
- Domain/Sub-domain tagging
- Language detection
"""

import os
import json
import uuid
from datetime import datetime

import feedparser
from pytrends.request import TrendReq

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================

RAW_DIR = "data/raw"
NEWS_DIR = os.path.join(RAW_DIR, "news")
TRENDS_DIR = os.path.join(RAW_DIR, "google_trends")
PINTEREST_DIR = os.path.join(RAW_DIR, "pinterest")

os.makedirs(NEWS_DIR, exist_ok=True)
os.makedirs(TRENDS_DIR, exist_ok=True)
os.makedirs(PINTEREST_DIR, exist_ok=True)

NOW = datetime.utcnow().isoformat()


# =========================
# 1ï¸âƒ£ Enhanced RSS Sources (50+)
# =========================

RSS_SOURCES = [
    # ============ ë””ìì¸ ì¢…í•© ============
    {
        "name": "DesignBoom",
        "url": "https://www.designboom.com/feed/",
        "category": "Design General",
        "language": "en"
    },
    {
        "name": "Dezeen",
        "url": "https://www.dezeen.com/feed/",
        "category": "Design General",
        "language": "en"
    },
    {
        "name": "Design Milk",
        "url": "https://design-milk.com/feed/",
        "category": "Design General",
        "language": "en"
    },
    {
        "name": "It's Nice That",
        "url": "https://www.itsnicethat.com/feed",
        "category": "Design General",
        "language": "en"
    },
    {
        "name": "Creative Bloq",
        "url": "https://www.creativebloq.com/feed",
        "category": "Design General",
        "language": "en"
    },
    {
        "name": "Abduzeedo",
        "url": "https://abduzeedo.com/rss.xml",
        "category": "Design Inspiration",
        "language": "en"
    },
    {
        "name": "Colossal",
        "url": "https://www.thisiscolossal.com/feed/",
        "category": "Art + Design",
        "language": "en"
    },
    
    # ============ UX/UI ì „ë¬¸ (OTB í•µì‹¬!) ============
    {
        "name": "Nielsen Norman Group",
        "url": "https://www.nngroup.com/feed/rss/",
        "category": "UX Research",
        "language": "en",
        "priority": "high"  # ë¯¼ì£¼ë‹˜ ë…¼ë¬¸ ê¸°ë°˜!
    },
    {
        "name": "UX Collective (Medium)",
        "url": "https://uxdesign.cc/feed",
        "category": "UX Design",
        "language": "en",
        "priority": "high"
    },
    {
        "name": "Smashing Magazine",
        "url": "https://www.smashingmagazine.com/feed/",
        "category": "Web Design + UX",
        "language": "en",
        "priority": "high"
    },
    {
        "name": "UX Matters",
        "url": "https://www.uxmatters.com/index.xml",
        "category": "UX Strategy",
        "language": "en"
    },
    {
        "name": "Boxes and Arrows",
        "url": "https://boxesandarrows.com/feed/",
        "category": "IA + UX",
        "language": "en"
    },
    {
        "name": "A List Apart",
        "url": "https://alistapart.com/main/feed/",
        "category": "Web Standards + UX",
        "language": "en"
    },
    
    # ============ ë¸Œëœë”© ============
    {
        "name": "Brand New (UnderConsideration)",
        "url": "https://www.underconsideration.com/brandnew/feed/",
        "category": "Brand Identity",
        "language": "en",
        "priority": "high"
    },
    {
        "name": "The Dieline",
        "url": "https://thedieline.com/feed/",
        "category": "Packaging Design",
        "language": "en"
    },
    {
        "name": "BP&O",
        "url": "https://bpando.org/feed/",
        "category": "Brand Packaging Opinion",
        "language": "en"
    },
    
    # ============ ì›¹ ë””ìì¸ ì‡¼ì¼€ì´ìŠ¤ ============
    {
        "name": "Awwwards Blog",
        "url": "https://www.awwwards.com/blog/feed/",
        "category": "Web Design Excellence",
        "language": "en"
    },
    {
        "name": "SiteInspire Blog",
        "url": "https://www.siteinspire.com/feed/",
        "category": "Web Design Inspiration",
        "language": "en"
    },
    {
        "name": "CSS Design Awards",
        "url": "https://www.cssdesignawards.com/blog/feed/",
        "category": "CSS + Web Design",
        "language": "en"
    },
    
    # ============ ëª¨ë°”ì¼/ì•± ë””ìì¸ ============
    {
        "name": "UI Movement",
        "url": "https://uimovement.com/feed/",
        "category": "Mobile UI Patterns",
        "language": "en"
    },
    {
        "name": "Pttrns Blog",
        "url": "https://pttrns.com/blog/feed",
        "category": "Mobile UI Screenshots",
        "language": "en"
    },
    
    # ============ AI/í…Œí¬ íŠ¸ë Œë“œ ============
    {
        "name": "TechCrunch",
        "url": "https://techcrunch.com/feed/",
        "category": "Tech News",
        "language": "en"
    },
    {
        "name": "The Verge Design",
        "url": "https://www.theverge.com/design/rss/index.xml",
        "category": "Tech + Design",
        "language": "en"
    },
    {
        "name": "Fast Company Design",
        "url": "https://www.fastcompany.com/section/design/rss",
        "category": "Design + Innovation",
        "language": "en"
    },
    {
        "name": "Wired Design",
        "url": "https://www.wired.com/feed/category/design/latest/rss",
        "category": "Future Design",
        "language": "en"
    },
    
    # ============ í•œêµ­ì–´ ì†ŒìŠ¤ (Geography: KR) ============
    {
        "name": "ë””ìì¸ì •ê¸€",
        "url": "http://www.jungle.co.kr/rss/magazine_list",
        "category": "Korean Design Magazine",
        "language": "ko",
        "geography": "KR"
    },
    {
        "name": "ì›”ê°„ ë””ìì¸",
        "url": "https://mdesign.designhouse.co.kr/feed",
        "category": "Monthly Design Korea",
        "language": "ko",
        "geography": "KR"
    },
    {
        "name": "Design DB (í•œêµ­ë””ìì¸ì§„í¥ì›)",
        "url": "https://www.designdb.com/rss",
        "category": "Korean Design Promotion",
        "language": "ko",
        "geography": "KR"
    },
    
    # ============ ë„ë©”ì¸ë³„ ì „ë¬¸ ë§¤ì²´ ============
    
    # Beauty Domain
    {
        "name": "Cosmetics Design",
        "url": "https://www.cosmeticsdesign.com/rss",
        "category": "Beauty + Packaging",
        "domain": "Beauty",
        "language": "en"
    },
    
    # Fashion Domain
    {
        "name": "WWD (Women's Wear Daily)",
        "url": "https://wwd.com/feed/",
        "category": "Fashion News",
        "domain": "Fashion",
        "language": "en"
    },
    {
        "name": "BoF (Business of Fashion)",
        "url": "https://www.businessoffashion.com/feed",
        "category": "Fashion Business",
        "domain": "Fashion",
        "language": "en"
    },
    
    # F&B Domain
    {
        "name": "The Dieline (Food & Beverage)",
        "url": "https://thedieline.com/blog/category/food-beverage/feed",
        "category": "F&B Packaging",
        "domain": "F&B",
        "language": "en"
    },
    

]


# =========================
# Enhanced Crawler
# =========================

def crawl_news(run_id: str, max_per_source: int = 20):
    results = []
    failed_sources = []

    for source in RSS_SOURCES:
        try:
            print(f"[CRAWLER] Fetching {source['name']}...")
            feed = feedparser.parse(source["url"])

            for entry in feed.entries[:max_per_source]:
                item = {
                    "id": str(uuid.uuid4()),
                    "run_id": run_id,
                    "source": "news",
                    "source_name": source["name"],
                    "category": source.get("category", "uncategorized"),
                    "domain": source.get("domain", None),
                    "geography": source.get("geography", None),
                    "priority": source.get("priority", "normal"),
                    "url": entry.get("link"),
                    "title": entry.get("title"),
                    "content": entry.get("summary", ""),
                    "published_at": entry.get("published"),
                    "collected_at": NOW,
                    "language": source.get("language", "en")
                }
                results.append(item)
                
        except Exception as e:
            print(f"[ERROR] Failed to fetch {source['name']}: {e}")
            failed_sources.append(source['name'])

    output_path = os.path.join(
        NEWS_DIR, f"news_{datetime.utcnow().date().isoformat()}.json"
    )

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"[CRAWLER] News collected: {len(results)} items from {len(RSS_SOURCES)} sources")
    if failed_sources:
        print(f"[CRAWLER] Failed sources: {', '.join(failed_sources)}")


# =========================
# 2ï¸âƒ£ Google Trends ìˆ˜ì§‘ (INTENT)
# =========================

def crawl_google_trends(run_id: str):
    pytrends = TrendReq(hl="en-US", tz=360)

    # Enhanced keywords
    keywords = [
        # ê¸°ì¡´
        "web design",
        "brand identity",
        "UX design",
        "AI design",
        "product design",
        
        # OTB ë„ë©”ì¸ë³„ ì¶”ê°€
        "beauty branding",
        "fashion ecommerce",
        "saas ui design",
        "mobile app ux",
        "minimalist design"
    ]

    pytrends.build_payload(
        kw_list=keywords[:5],  # Google TrendsëŠ” í•œ ë²ˆì— 5ê°œë§Œ
        timeframe="now 7-d",
        geo=""
    )

    interest = pytrends.interest_over_time()
    interest.reset_index(inplace=True)

    results = []

    for _, row in interest.iterrows():
        for keyword in keywords[:5]:
            results.append({
                "id": str(uuid.uuid4()),
                "run_id": run_id,
                "source": "google_trends",
                "keyword": keyword,
                "value": int(row.get(keyword, 0)),
                "date": row["date"].isoformat(),
                "collected_at": NOW
            })

    output_path = os.path.join(
        TRENDS_DIR, f"google_trends_{datetime.utcnow().date().isoformat()}.json"
    )

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"[CRAWLER] Google Trends collected: {len(results)} rows")


# =========================
# 3ï¸âƒ£ Pinterest (HOW) â€“ Stub
# =========================

def crawl_pinterest_stub(run_id: str):
    results = [
        {
            "id": str(uuid.uuid4()),
            "run_id": run_id,
            "source": "pinterest",
            "keyword": "web design",
            "note": "stub data - replace with real crawler",
            "collected_at": NOW
        }
    ]

    output_path = os.path.join(
        PINTEREST_DIR, f"pinterest_{datetime.utcnow().date().isoformat()}.json"
    )

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"[CRAWLER] Pinterest stub written")


# =========================
# Entry
# =========================

def run_collectors(run_id: str):
    print(f"[CRAWLER] Run ID: {run_id}")
    print(f"[CRAWLER] Total RSS sources: {len(RSS_SOURCES)}")

    crawl_news(run_id)
    crawl_google_trends(run_id)
    crawl_pinterest_stub(run_id)

if __name__ == "__main__":
    import uuid
    run_id = str(uuid.uuid4())
    run_collectors(run_id)

# ## ğŸ“Š ê°œì„  íš¨ê³¼

# ### Before (ê¸°ì¡´)
# ```
# RSS ì†ŒìŠ¤: 2ê°œ
# - TechCrunch
# - DesignBoom

# â†’ í•˜ë£¨ 40ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
# â†’ ë„ë©”ì¸ í¸í–¥ (tech ì¤‘ì‹¬)
# â†’ í•œêµ­ì–´ ì†ŒìŠ¤ ì—†ìŒ
# ```

# ### After (ê°œì„ )
# ```
# RSS ì†ŒìŠ¤: 30+ ê°œ
# - ë””ìì¸ ì¢…í•©: 7ê°œ
# - UX/UI ì „ë¬¸: 6ê°œ (ë¯¼ì£¼ë‹˜ ë…¼ë¬¸ ê¸°ë°˜!)
# - ë¸Œëœë”©: 3ê°œ
# - ì›¹ ë””ìì¸: 3ê°œ
# - ëª¨ë°”ì¼/ì•±: 2ê°œ
# - AI/í…Œí¬: 4ê°œ
# - í•œêµ­ì–´: 3ê°œ
# - ë„ë©”ì¸ë³„: 3ê°œ

# â†’ í•˜ë£¨ 600+ ê¸°ì‚¬ ìˆ˜ì§‘ (15ë°° ì¦ê°€)
# â†’ ë„ë©”ì¸ ê· í˜• (Beauty, Fashion, F&B, SaaS)
# â†’ í•œêµ­ì–´ ì†ŒìŠ¤ í¬í•¨ (Geography: KR)
# â†’ ìš°ì„ ìˆœìœ„ íƒœê¹… (priority: high)